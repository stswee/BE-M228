{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset parameters\n",
    "num_samples = 32  # Small subset for testing\n",
    "embedding_dim = 768  # Dimension of ECG and text embeddings\n",
    "num_classes = 3  # Tertiary classification\n",
    "\n",
    "# Generate random ECG and text embeddings\n",
    "np.random.seed(42)\n",
    "ecg_embeddings = np.random.randn(num_samples, embedding_dim).astype(np.float32)\n",
    "text_embeddings = np.random.randn(num_samples, embedding_dim).astype(np.float32)\n",
    "\n",
    "# Generate synthetic class labels (random integers between 0 and 2)\n",
    "labels = np.random.randint(0, num_classes, size=(num_samples,))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "ecg_embeddings = torch.tensor(ecg_embeddings)\n",
    "text_embeddings = torch.tensor(text_embeddings)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Create a simple dataset\n",
    "class ECGTextDataset(Dataset):\n",
    "    def __init__(self, ecg_data, text_data, labels):\n",
    "        self.ecg_data = ecg_data\n",
    "        self.text_data = text_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ecg_data[idx], self.text_data[idx], self.labels[idx]\n",
    "\n",
    "# Instantiate dataset and dataloader for the small batch\n",
    "dataset = ECGTextDataset(ecg_embeddings, text_embeddings, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier for text embeddings\n",
    "class MLPText(nn.Module):\n",
    "    def __init__(self, input_dim = 768, hidden_dim = 256, num_classes = 3):\n",
    "        super(MLPText, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# MLP Classifier for ECG embeddings\n",
    "class MLPECG(nn.Module):\n",
    "    def __init__(self, input_dim = 768, hidden_dim = 256, num_classes = 3):\n",
    "        super(MLPECG, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/sswee/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "ecg_mlp = MLPECG(input_dim = embedding_dim, hidden_dim = 256, num_classes = num_classes)\n",
    "text_mlp = MLPText(input_dim = embedding_dim, hidden_dim = 256, num_classes = num_classes)\n",
    "\n",
    "# Initialize optimizer and loss functions\n",
    "optimizer = torch.optim.Adam(list(ecg_mlp.parameters()) + list(text_mlp.parameters()), lr=0.001)\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4/4 [00:12<00:00,  3.17s/it, kl_loss=0.0111, prediction_loss=0.0459, total_loss=0.0515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Prediction Loss=0.0459, KL Loss=0.0111, Total Loss=0.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 4/4 [00:12<00:00,  3.20s/it, kl_loss=0.00519, prediction_loss=0.0427, total_loss=0.0453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Prediction Loss=0.0427, KL Loss=0.0052, Total Loss=0.0453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 4/4 [00:12<00:00,  3.22s/it, kl_loss=0.00562, prediction_loss=0.0463, total_loss=0.0491]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Prediction Loss=0.0463, KL Loss=0.0056, Total Loss=0.0491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    ecg_mlp.train()\n",
    "    text_mlp.train()\n",
    "    \n",
    "    with tqdm(dataloader, desc = f\"Epoch {epoch + 1}\", total = len(dataloader)) as pbar:\n",
    "        for ecg_batch, text_batch, label_batch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            ecg_outputs = ecg_mlp(ecg_batch) # [batch_size, num_classes]\n",
    "            log_ecg_outputs = F.log_softmax(ecg_outputs, dim = 1)\n",
    "            text_outputs = text_mlp(text_batch) # [batch_size, num_classes]\n",
    "            soft_text_outputs = F.softmax(text_outputs, dim = 1)\n",
    "            \n",
    "            # Compute losses\n",
    "            prediction_loss = classification_loss(ecg_outputs, label_batch)\n",
    "            kl_divergence_loss = kl_loss(log_ecg_outputs, soft_text_outputs)\n",
    "            \n",
    "            # Total loss\n",
    "            alpha = 0.5 # Weight for KL divergence loss\n",
    "            total_loss = prediction_loss + alpha * kl_divergence_loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update\n",
    "            pbar.set_postfix(prediction_loss=prediction_loss.item(), kl_loss=kl_divergence_loss.item(), total_loss=total_loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Prediction Loss={prediction_loss.item():.4f}, KL Loss={kl_divergence_loss.item():.4f}, Total Loss={total_loss.item():.4f}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_llm",
   "language": "python",
   "name": "ecg_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
