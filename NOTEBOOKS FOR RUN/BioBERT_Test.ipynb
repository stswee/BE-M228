{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/sswee/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "model = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Encode words\n",
    "def encode_text(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Forward pass to get hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token representation (sentence embedding)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return cls_embedding\n",
    "\n",
    "# Example usage\n",
    "text = \"The patient's heart is fine.\"\n",
    "vector = encode_text(text)\n",
    "print(vector.shape)  # Should be (1, 768) for BioBERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding\n",
    "def get_cls_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] embedding\n",
    "    return cls_embedding  # Shape: (1, 768)\n",
    "\n",
    "# Toy dataset\n",
    "texts = [\"Heart failure detected.\", \"No signs of cardiovascular issues.\", \"Possible arrhythmia found.\"]\n",
    "labels = [1, 0, 1]  # 1: Disease, 0: No Disease\n",
    "\n",
    "# Convert texts to embeddings\n",
    "embeddings = torch.cat([get_cls_embedding(text) for text in texts]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim = 768, hidden_dim = 256, num_classes = 2):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP\n",
    "# Example medical sentences and labels (1 = disease, 0 = no disease)\n",
    "texts = [\"Patient shows signs of arrhythmia.\", \n",
    "         \"No signs of cardiovascular issues.\", \n",
    "         \"ECG indicates possible heart failure.\", \n",
    "         \"Heart rate appears normal.\"]\n",
    "\n",
    "labels = torch.tensor([1, 0, 1, 0])  # Binary classification\n",
    "\n",
    "# Convert texts to embeddings\n",
    "embeddings = torch.cat([get_cls_embedding(text) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset\n",
    "dataset = TensorDataset(embeddings, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Initialize MLP\n",
    "model = MLPClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it, loss=0.697]\n",
      "Epoch 2/10: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it, loss=0.798]\n",
      "Epoch 3/10: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it, loss=0.63] \n",
      "Epoch 4/10: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it, loss=0.459]\n",
      "Epoch 5/10: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it, loss=0.369]\n",
      "Epoch 6/10: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it, loss=0.382]\n",
      "Epoch 7/10: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it, loss=0.322]\n",
      "Epoch 8/10: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it, loss=0.208]\n",
      "Epoch 9/10: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it, loss=0.31] \n",
      "Epoch 10/10: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it, loss=0.184]\n"
     ]
    }
   ],
   "source": [
    "# Training loop with mini-batches\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pbar.set_postfix(loss=loss.item())  # Update progress bar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_llm",
   "language": "python",
   "name": "ecg_llm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
